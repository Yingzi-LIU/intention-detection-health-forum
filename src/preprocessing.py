import pandas as pd
import re
import emoji
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.preprocessing import LabelEncoder
import nltk
import os

# ä¸‹è½½ NLTK èµ„æº
nltk.download('stopwords')
nltk.download('wordnet')

def load_data(base_path):
    """
    åŠ è½½æ•°æ®é›†
    """
    try:
        train_df = pd.read_csv(base_path + 'train_dataset.csv')
        val_df = pd.read_csv(base_path + 'validation_dataset.csv')
        test_df = pd.read_csv(base_path + 'test_dataset.csv')
        print("âœ… All datasets loaded successfully.")
        return train_df, val_df, test_df
    except FileNotFoundError:
        print(f"âŒ Error: One or more dataset files not found at '{base_path}'.")
        return None, None, None

# åŸå§‹æ³•è¯­åœç”¨è¯
french_stopwords = set(stopwords.words('french'))
extra_french_stopwords = {
    "le", "la", "les", "un", "une", "des", "ce", "Ã§a", "celui", "ceux",
    "qui", "que", "quoi", "je", "tu", "il", "elle", "nous", "vous", "ils",
    "elles", "me", "te", "se", "lui", "leur", "et", "ou", "mais", "donc",
    "car", "ni", "Ã ", "de", "en", "pour", "avec", "sans", "sur", "sous",
    "chez", "dans", "du", "au", "aux"
}
french_stopwords.update(extra_french_stopwords)
lemmatizer = WordNetLemmatizer()

# ğŸ”¹ é¢œæ–‡å­—/emoji æ˜ å°„è¡¨
emoticon_dict = {
    ":)": "EMOJI_SMILE", ":-)": "EMOJI_SMILE", ":(": "EMOJI_SAD", ":-(": "EMOJI_SAD",
    ";)": "EMOJI_WINK", ":-D": "EMOJI_LAUGH", ":D": "EMOJI_LAUGH", "<3": "EMOJI_HEART"
}

def replace_emoticons(text):
    for emot, token in emoticon_dict.items():
        text = text.replace(emot, f" {token} ")
    return text

def replace_emojis(text):
    def _replace(e, data=None):
        name = emoji.demojize(e).upper().strip(":")
        return f" EMOJI_{name} "
    return emoji.replace_emoji(text, replace=_replace)

def clean_text(text, remove_stopwords=False):
    """ä¸»æ–‡æœ¬æ¸…æ´—å‡½æ•°"""
    text = str(text).lower()
    text = replace_emoticons(text)
    text = replace_emojis(text)
    text = re.sub(r"<.*?>", " ", text)
    text = re.sub(r"http\S+|www\S+", " URL ", text)
    text = re.sub(r"#\w+", " ", text)
    text = re.sub(r"@\w+", " ", text)
    # ç§»é™¤æ‰€æœ‰æ ‡ç‚¹ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦
    text = re.sub(r"[^a-zA-ZÃ€-Ã¿\s\d]", " ", text)
    text = re.sub(r"\d+", " NUM ", text)
    text = re.sub(r"\s+", " ", text).strip()
    words = text.split()
    words = [lemmatizer.lemmatize(w) for w in words]
    if remove_stopwords:
        words = [w for w in words if w not in french_stopwords]
    return " ".join(words)

def preprocess_dataset(df, text_col="Sentences", min_tokens=2, remove_stopwords=False):
    """å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œé¢„å¤„ç†"""
    df[text_col + "_clean"] = df[text_col].apply(lambda x: clean_text(x, remove_stopwords))
    df = df.drop_duplicates(subset=[text_col + "_clean"])
    df = df[df[text_col + "_clean"].apply(lambda x: len(x.split()) >= min_tokens)]
    return df

# å°†è¿™éƒ¨åˆ†æ”¾åœ¨ä½ çš„preprocessing.pyæ–‡ä»¶ä¸­ï¼Œæ›¿æ¢æ‰åŸæ¥çš„encode_labelså‡½æ•°
def encode_labels(train_df, val_df, test_df, label_col_map):
    """
    ä½¿ç”¨LabelEncoderå¯¹æŒ‡å®šçš„æ ‡ç­¾åˆ—è¿›è¡Œç¼–ç ã€‚
    
    Args:
        train_df, val_df, test_df (pd.DataFrame): åŒ…å«æ ‡ç­¾åˆ—çš„æ•°æ®é›†ã€‚
        label_col_map (dict): å­—å…¸ï¼Œé”®æ˜¯ä»»åŠ¡åç§°ï¼Œå€¼æ˜¯å®é™…çš„æ•°æ®åˆ—åã€‚
    
    Returns:
        tuple: (train_df, val_df, test_df, label_encoders) åŒ…å«ç¼–ç æ ‡ç­¾å’Œç¼–ç å™¨çš„å…ƒç»„ã€‚
    """
    label_encoders = {}
    
    # éå†å­—å…¸ä¸­çš„å€¼ï¼Œå³å®é™…çš„åˆ—å
    for task_name, original_col in label_col_map.items(): 
        # ç¡®ä¿åˆ—ååœ¨DataFrameä¸­å­˜åœ¨
        if original_col not in train_df.columns:
            print(f"âŒ Error: Column '{original_col}' not found in the DataFrame. Skipping encoding for '{task_name}'.")
            continue

        le = LabelEncoder()
        
        # âš ï¸ å…³é”®ä¿®å¤ï¼šä½¿ç”¨åŸå§‹åˆ—åï¼ˆoriginal_colï¼‰æ¥è®¿é—®DataFrame
        all_labels = pd.concat([train_df[original_col], val_df[original_col], test_df[original_col]]).astype(str).unique()
        le.fit(all_labels)
        
        # åŒæ ·ï¼Œåœ¨è½¬æ¢æ—¶ä¹Ÿä½¿ç”¨åŸå§‹åˆ—å
        train_df[original_col] = le.transform(train_df[original_col].astype(str))
        val_df[original_col] = le.transform(val_df[original_col].astype(str))
        test_df[original_col] = le.transform(test_df[original_col].astype(str))
        
        label_encoders[task_name] = le

    if not label_encoders:
        print("âš ï¸ No labels were encoded. Please check your label column names.")

    print("âœ… Labels encoded successfully.")
    return train_df, val_df, test_df, label_encoders

def save_cleaned_data(train_df, val_df, test_df, base_path):
    """ä¿å­˜æ¸…ç†å¹¶ç¼–ç åçš„æ•°æ®é›†"""
    try:
        os.makedirs(base_path, exist_ok=True)
        train_path = os.path.join(base_path, "train_dataset_clean.csv")
        val_path = os.path.join(base_path, "validation_dataset_clean.csv")
        test_path = os.path.join(base_path, "test_dataset_clean.csv")
        
        train_df.to_csv(train_path, index=False, encoding="utf-8")
        val_df.to_csv(val_path, index=False, encoding="utf-8")
        test_df.to_csv(test_path, index=False, encoding="utf-8")
        
        print("\nğŸ“‚ Cleaned datasets saved successfully.")
    except Exception as e:
        print(f"âŒ Error saving cleaned data: {e}")

###ç‰¹å¾å¢å¼º

def add_keyword_features(df, keyword_dict):
    """
    Adds new boolean features to the DataFrame based on the presence of keywords.

    Args:
        df (pd.DataFrame): The DataFrame with a 'Sentences_clean' column.
        keyword_dict (dict): A nested dictionary of keywords for each task and label.

    Returns:
        pd.DataFrame: The DataFrame with new keyword feature columns.
    """
    # Create an empty DataFrame to store the new features
    new_features = pd.DataFrame(index=df.index)

    # Flatten the keyword dictionary to iterate through all labels
    for task_name, labels_dict in keyword_dict.items():
        for label_name, keywords in labels_dict.items():
            # Create a unique column name for this keyword feature
            column_name = f'keyword_{task_name}_{label_name}'
            
            # Check if any of the keywords for the current label are in the clean sentence
            new_features[column_name] = df['Sentences_clean'].apply(
                lambda x: any(keyword in x for keyword in keywords)
            ).astype(int)

    return pd.concat([df, new_features], axis=1)